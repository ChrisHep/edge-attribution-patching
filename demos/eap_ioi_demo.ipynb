{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6606875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aoq559/dev/transformer/eap/edge-attribution-patching\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60758/3747139492.py:6: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"%load_ext autoreload\")\n",
      "/tmp/ipykernel_60758/3747139492.py:7: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"%autoreload 2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "%cd /home/aoq559/dev/transformer/eap/edge-attribution-patching\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "if ipython is not None:\n",
    "    ipython.magic(\"%load_ext autoreload\")\n",
    "    ipython.magic(\"%autoreload 2\")\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import einops\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "from eapp.eap_wrapper import EAP\n",
    "\n",
    "from jaxtyping import Float\n",
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a16eab",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20df2bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt-neo-125M into HookedTransformer\n",
      "Using tokenizer GPT2TokenizerFast(name_or_path='EleutherAI/gpt-neo-125m', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# model = HookedTransformer.from_pretrained(\n",
    "#     'EleutherAI/pythia-12b-deduped-v0',\n",
    "#     center_writing_weights=False,\n",
    "#     center_unembed=False,\n",
    "#     fold_ln=False,\n",
    "#     device=device,\n",
    "#     n_devices=7,\n",
    "#     move_to_device=True,\n",
    "#     dtype='float16'\n",
    "# )\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    'gpt-neo-125M',\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    "    n_devices=5,\n",
    "    move_to_device=True,\n",
    "    dtype='float16'\n",
    ")\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "print(f\"Using tokenizer {tokenizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4cad0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/shared-network/shared/2024_ml_master/data/EleutherAI/gpt-neo-125m/intervention_1_shots_max_20_arabic_further_templates.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from /shared-network/shared/2024_ml_master/data/EleutherAI/gpt-neo-125m/intervention_1_shots_max_20_arabic_further_templates.pkl\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import yaml\n",
    "import pickle\n",
    "import os\n",
    "class DotDict(dict):\n",
    "    \"\"\" Dot notation access to dictionary attributes \"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "yaml_file_path = \"./conf/config.yaml\"\n",
    "with open(yaml_file_path, \"r\") as f:\n",
    "    args = DotDict(yaml.safe_load(f))\n",
    "\n",
    "file_name = args.data_dir\n",
    "file_name += '/' + str(args.model)\n",
    "file_name += '/intervention_' + str(args.n_shots) + '_shots_max_' + str(args.max_n) + '_' + args.representation\n",
    "file_name += '_further_templates' if args.extended_templates else ''\n",
    "file_name += '.pkl'\n",
    "print(file_name)\n",
    "\n",
    "with open(file_name, 'rb') as f:\n",
    "    intervention_list = pickle.load(f)\n",
    "print(\"Loaded data from\", file_name)\n",
    "if args.debug_run:\n",
    "    intervention_list = intervention_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292dfbf6",
   "metadata": {},
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "601a7d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                      Sentences from IOI vs ABC distribution                                       </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> IOI prompt                              </span>┃<span style=\"font-weight: bold\"> IOI subj </span>┃<span style=\"font-weight: bold\"> IOI indirect obj </span>┃<span style=\"font-weight: bold\"> ABC prompt                              </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Victoria</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jane</span> got a snack at   │ Jane     │ Victoria         │ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Victoria</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jane</span> got a snack at   │\n",
       "│ the store, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jane</span> decided to give it to   │          │                  │ the store, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jane</span> decided to give it to   │\n",
       "│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Victoria</span>                                │          │                  │ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Victoria</span>                                │\n",
       "│                                         │          │                  │                                         │\n",
       "│ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Sullivan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Rose</span> got a necklace   │ Sullivan │ Rose             │ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Sullivan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Rose</span> got a necklace   │\n",
       "│ at the garden, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Sullivan</span> decided to give │          │                  │ at the garden, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Sullivan</span> decided to give │\n",
       "│ it to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Rose</span>                              │          │                  │ it to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Rose</span>                              │\n",
       "│                                         │          │                  │                                         │\n",
       "│ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alex</span> got a drink at the   │ Alex     │ Alan             │ When <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alex</span> got a drink at the   │\n",
       "│ store, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alex</span> decided to give it to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alan</span>  │          │                  │ store, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alex</span> decided to give it to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Alan</span>  │\n",
       "│                                         │          │                  │                                         │\n",
       "│ Then, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jessica</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Crystal</span> had a long    │ Jessica  │ Crystal          │ Then, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jessica</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Crystal</span> had a long    │\n",
       "│ argument, and afterwards <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jessica</span> said   │          │                  │ argument, and afterwards <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jessica</span> said   │\n",
       "│ to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Crystal</span>                              │          │                  │ to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Crystal</span>                              │\n",
       "│                                         │          │                  │                                         │\n",
       "│ Then, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jonathan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Kevin</span> were working   │ Kevin    │ Jonathan         │ Then, <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jonathan</span> and <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Kevin</span> were working   │\n",
       "│ at the school. <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Kevin</span> decided to give a  │          │                  │ at the school. <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Kevin</span> decided to give a  │\n",
       "│ necklace to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jonathan</span>                    │          │                  │ necklace to <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">Jonathan</span>                    │\n",
       "│                                         │          │                  │                                         │\n",
       "└─────────────────────────────────────────┴──────────┴──────────────────┴─────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                      Sentences from IOI vs ABC distribution                                       \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mIOI prompt                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mIOI subj\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mIOI indirect obj\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mABC prompt                             \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ When \u001b[1;4;38;5;208mVictoria\u001b[0m and \u001b[1;4;38;5;208mJane\u001b[0m got a snack at   │ Jane     │ Victoria         │ When \u001b[1;4;38;5;208mVictoria\u001b[0m and \u001b[1;4;38;5;208mJane\u001b[0m got a snack at   │\n",
       "│ the store, \u001b[1;4;38;5;208mJane\u001b[0m decided to give it to   │          │                  │ the store, \u001b[1;4;38;5;208mJane\u001b[0m decided to give it to   │\n",
       "│ \u001b[1;4;38;5;208mVictoria\u001b[0m                                │          │                  │ \u001b[1;4;38;5;208mVictoria\u001b[0m                                │\n",
       "│                                         │          │                  │                                         │\n",
       "│ When \u001b[1;4;38;5;208mSullivan\u001b[0m and \u001b[1;4;38;5;208mRose\u001b[0m got a necklace   │ Sullivan │ Rose             │ When \u001b[1;4;38;5;208mSullivan\u001b[0m and \u001b[1;4;38;5;208mRose\u001b[0m got a necklace   │\n",
       "│ at the garden, \u001b[1;4;38;5;208mSullivan\u001b[0m decided to give │          │                  │ at the garden, \u001b[1;4;38;5;208mSullivan\u001b[0m decided to give │\n",
       "│ it to \u001b[1;4;38;5;208mRose\u001b[0m                              │          │                  │ it to \u001b[1;4;38;5;208mRose\u001b[0m                              │\n",
       "│                                         │          │                  │                                         │\n",
       "│ When \u001b[1;4;38;5;208mAlan\u001b[0m and \u001b[1;4;38;5;208mAlex\u001b[0m got a drink at the   │ Alex     │ Alan             │ When \u001b[1;4;38;5;208mAlan\u001b[0m and \u001b[1;4;38;5;208mAlex\u001b[0m got a drink at the   │\n",
       "│ store, \u001b[1;4;38;5;208mAlex\u001b[0m decided to give it to \u001b[1;4;38;5;208mAlan\u001b[0m  │          │                  │ store, \u001b[1;4;38;5;208mAlex\u001b[0m decided to give it to \u001b[1;4;38;5;208mAlan\u001b[0m  │\n",
       "│                                         │          │                  │                                         │\n",
       "│ Then, \u001b[1;4;38;5;208mJessica\u001b[0m and \u001b[1;4;38;5;208mCrystal\u001b[0m had a long    │ Jessica  │ Crystal          │ Then, \u001b[1;4;38;5;208mJessica\u001b[0m and \u001b[1;4;38;5;208mCrystal\u001b[0m had a long    │\n",
       "│ argument, and afterwards \u001b[1;4;38;5;208mJessica\u001b[0m said   │          │                  │ argument, and afterwards \u001b[1;4;38;5;208mJessica\u001b[0m said   │\n",
       "│ to \u001b[1;4;38;5;208mCrystal\u001b[0m                              │          │                  │ to \u001b[1;4;38;5;208mCrystal\u001b[0m                              │\n",
       "│                                         │          │                  │                                         │\n",
       "│ Then, \u001b[1;4;38;5;208mJonathan\u001b[0m and \u001b[1;4;38;5;208mKevin\u001b[0m were working   │ Kevin    │ Jonathan         │ Then, \u001b[1;4;38;5;208mJonathan\u001b[0m and \u001b[1;4;38;5;208mKevin\u001b[0m were working   │\n",
       "│ at the school. \u001b[1;4;38;5;208mKevin\u001b[0m decided to give a  │          │                  │ at the school. \u001b[1;4;38;5;208mKevin\u001b[0m decided to give a  │\n",
       "│ necklace to \u001b[1;4;38;5;208mJonathan\u001b[0m                    │          │                  │ necklace to \u001b[1;4;38;5;208mJonathan\u001b[0m                    │\n",
       "│                                         │          │                  │                                         │\n",
       "└─────────────────────────────────────────┴──────────┴──────────────────┴─────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ioi_dataset import IOIDataset, format_prompt, make_table\n",
    "N = 25\n",
    "clean_dataset = IOIDataset(\n",
    "    prompt_type='mixed',\n",
    "    N=N,\n",
    "    tokenizer=model.tokenizer,\n",
    "    prepend_bos=False,\n",
    "    seed=1,\n",
    "    device=device\n",
    ")\n",
    "corr_dataset = clean_dataset.gen_flipped_prompts('ABC->XYZ, BAB->XYZ')\n",
    "\n",
    "make_table(\n",
    "  colnames = [\"IOI prompt\", \"IOI subj\", \"IOI indirect obj\", \"ABC prompt\"],\n",
    "  cols = [\n",
    "    map(format_prompt, clean_dataset.sentences),\n",
    "    model.to_string(clean_dataset.s_tokenIDs).split(),\n",
    "    model.to_string(clean_dataset.io_tokenIDs).split(),\n",
    "    map(format_prompt, clean_dataset.sentences),\n",
    "  ],\n",
    "  title = \"Sentences from IOI vs ABC distribution\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657f126",
   "metadata": {},
   "source": [
    "# Metric Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e71a358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1315]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'op3_pos': 18,\n",
       " 'operator_word': None,\n",
       " 'operands_alt': '2 7 6',\n",
       " 'operands_base': '2 7 6',\n",
       " 'operator_pos': None,\n",
       " 'op2_pos': 16,\n",
       " 'op1_pos': 14,\n",
       " 'res_alt_tok': [1315],\n",
       " 'res_base_tok': [1315],\n",
       " 'res_string': None,\n",
       " 'res_base_string': '15',\n",
       " 'res_alt_string': '15',\n",
       " 'device': 'cpu',\n",
       " 'multitoken': False,\n",
       " 'is_llama': False,\n",
       " 'is_opt': False,\n",
       " 'is_bloom': False,\n",
       " 'is_mistral': False,\n",
       " 'is_persimmon': False,\n",
       " 'representation': 'arabic',\n",
       " 'extended_templates': True,\n",
       " 'template_id': '-',\n",
       " 'n_vars': 2,\n",
       " 'base_string': 'The result of 2 + 7 + 6 =',\n",
       " 'alt_string': 'The result of 2 + 7 + 6 =',\n",
       " 'few_shots': 'The result of 7 + 5 + 3 = 15. ',\n",
       " 'few_shots_t2': ' ',\n",
       " 'equation': '({x}+{y} + {z})',\n",
       " 'enc': GPT2Tokenizer(name_or_path='EleutherAI/gpt-neo-125m', vocab_size=50257, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " },\n",
       " 'len_few_shots': 12,\n",
       " 'len_few_shots_t2': 1,\n",
       " 'base_string_tok_list': [464,\n",
       "  1255,\n",
       "  286,\n",
       "  767,\n",
       "  1343,\n",
       "  642,\n",
       "  1343,\n",
       "  513,\n",
       "  796,\n",
       "  1315,\n",
       "  13,\n",
       "  383,\n",
       "  1255,\n",
       "  286,\n",
       "  362,\n",
       "  1343,\n",
       "  767,\n",
       "  1343,\n",
       "  718,\n",
       "  796],\n",
       " 'alt_string_tok_list': [220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  50256,\n",
       "  383,\n",
       "  1255,\n",
       "  286,\n",
       "  362,\n",
       "  1343,\n",
       "  767,\n",
       "  1343,\n",
       "  718,\n",
       "  796],\n",
       " 'base_string_tok': tensor([[ 464, 1255,  286,  767, 1343,  642, 1343,  513,  796, 1315,   13,  383,\n",
       "          1255,  286,  362, 1343,  767, 1343,  718,  796]]),\n",
       " 'alt_string_tok': tensor([[  220,   220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
       "          50256,   383,  1255,   286,   362,  1343,   767,  1343,   718,   796]]),\n",
       " 'pred_alt_string': ' 8',\n",
       " 'pred_res_alt_tok': [807]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intervention = intervention_list[0]\n",
    "print(intervention.res_base_tok)\n",
    "intervention.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d0ec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ -7.465  -6.3    -8.26  ... -13.19   -9.43   -6.61 ]\n",
      "  [ -7.74  -10.9   -15.34  ... -20.4   -18.86  -14.484]\n",
      "  [-12.34  -10.46  -13.24  ... -14.28  -12.445 -10.54 ]\n",
      "  ...\n",
      "  [-13.74  -12.58  -14.51  ... -22.11  -15.08  -10.7  ]\n",
      "  [-11.086 -12.234 -14.95  ... -23.89  -17.08  -11.43 ]\n",
      "  [-11.68  -11.29  -13.55  ... -18.84  -12.17   -7.54 ]]]\n",
      "1315\n",
      "[-0.77]\n",
      "[0.03464]\n"
     ]
    }
   ],
   "source": [
    "intervention = intervention_list[0]\n",
    "with torch.no_grad():\n",
    "    clean_logits = model(intervention.base_string_tok).cpu().numpy()\n",
    "    print(clean_logits)\n",
    "    corrupt_logits = model(intervention.alt_string_tok).cpu().numpy()\n",
    "    clean_logits_argmax = np.argmax(clean_logits, axis=2)[0, -1]\n",
    "    print(clean_logits_argmax)\n",
    "    clean_logit = clean_logits[:, -1, intervention.res_base_tok[0]]\n",
    "    corrupt_logit = corrupt_logits[:, -1, intervention.pred_res_alt_tok[0]]\n",
    "    logit_diff = corrupt_logit - clean_logit\n",
    "    print(clean_logit)\n",
    "    print(corrupt_logit)\n",
    "\n",
    "    \n",
    "    # clean_logit_argmax = torch.argmax(clean_logits, dim=2)\n",
    "    # corrupt_logit_argmax = torch.argmax(corrupt_logits, dim=2)\n",
    "    # print(clean_logit_argmax)\n",
    "    # next_word_index = clean_logit_argmax[0][-1]\n",
    "    # next_word = model.tokenizer.convert_ids_to_tokens(next_word_index.item())\n",
    "    # print(next_word)\n",
    "    \n",
    "    # clean_prediction = torch.argmax(clean_logits, dim=2)\n",
    "    # print(clean_prediction)\n",
    "    # next_word_index = clean_prediction[0][-1]\n",
    "    # next_word = tokenizer.convert_ids_to_tokens(next_word_index.item())\n",
    "    # print(next_word_index)\n",
    "    # print(next_word)\n",
    "    # print(torch.argmax(clean_logits[:, -1, :]))\n",
    "    # print(corrupt_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a02f4793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intervention_dataset\n",
    "intervention_data = intervention_dataset.InterventionDataset(intervention_list, device, model.tokenizer)\n",
    "intervention_data.create_intervention_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b51b2c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[  17, 1635,  362, 1635,  513,  796, 1105,   13,  362, 1635,  362, 1635,\n",
      "          513,  796],\n",
      "        [  17, 1635,  362, 1635,  362,  796,  807,   13,  362, 1635,  362, 1635,\n",
      "          362,  796],\n",
      "        [  17, 1635,  362, 1635,  604,  796, 1467,   13,  362, 1635,  604, 1635,\n",
      "          362,  796],\n",
      "        [  18, 1635,  513, 1635,  362,  796, 1248,   13,  362, 1635,  513, 1635,\n",
      "          513,  796],\n",
      "        [  17, 1635,  604, 1635,  362,  796, 1467,   13,  604, 1635,  362, 1635,\n",
      "          362,  796],\n",
      "        [  19, 1635,  362, 1635,  362,  796, 1467,   13,  362, 1635,  604, 1635,\n",
      "          362,  796],\n",
      "        [  18, 1635,  362, 1635,  513,  796, 1248,   13,  513, 1635,  513, 1635,\n",
      "          362,  796],\n",
      "        [  18, 1635,  513, 1635,  362,  796, 1248,   13,  513, 1635,  513, 1635,\n",
      "          362,  796],\n",
      "        [  19, 1635,  362, 1635,  362,  796, 1467,   13,  362, 1635,  362, 1635,\n",
      "          604,  796]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(intervention_data.base_string_toks):\n",
    "    batch = torch.vstack(batch)\n",
    "    print(batch_idx, batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8d87fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tensor = t.tensor([[  17, 1635,  362, 1635,  513,  796, 1105,   13,  362, 1635,  362, 1635,\n",
    "          513,  796],\n",
    "        [  17, 1635,  362, 1635,  362,  796,  807,   13,  362, 1635,  362, 1635,\n",
    "          362,  796]])\n",
    "example_tensor = example_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7142580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 14, 50257])\n",
      "tensor(-1.7578, device='cuda:5', dtype=torch.float16, grad_fn=<MeanBackward0>)\n",
      "clean_logit_diff_example: -1.7578125\n"
     ]
    }
   ],
   "source": [
    "clean_logits_example = model(example_tensor)\n",
    "print(clean_logits_example.shape)\n",
    "print(ave_logit_difference(clean_logits_example, intervention_data, False))\n",
    "clean_logit_diff_example = ave_logit_difference(clean_logits_example, intervention_data, per_prompt=False).item()\n",
    "print(f\"clean_logit_diff_example: {clean_logit_diff_example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ae31a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "clean_logits 1 shape torch.Size([22, 20, 50257])\n",
      "clean_logits_1 diff tensor([-1.7666, -1.4717, -1.6963, -1.8682, -6.6523,  5.2578, -1.9863, -1.8691,\n",
      "        -0.3564, -0.7568, -0.3042,  1.1621, -0.2988, -0.0527, -3.8613, -5.2930,\n",
      "        -4.7852, -6.2031, -4.1953, -5.2344, -5.2930, -4.2227], device='cuda:5',\n",
      "       dtype=torch.float16)\n",
      "[tensor([-1.5996, -1.9082, -2.8613, -1.4355, -4.3906, -3.3340, -3.3262, -3.9297,\n",
      "        -2.6582], device='cuda:5', dtype=torch.float16), tensor([-1.7666, -1.4717, -1.6963, -1.8682, -6.6523,  5.2578, -1.9863, -1.8691,\n",
      "        -0.3564, -0.7568, -0.3042,  1.1621, -0.2988, -0.0527, -3.8613, -5.2930,\n",
      "        -4.7852, -6.2031, -4.1953, -5.2344, -5.2930, -4.2227], device='cuda:5',\n",
      "       dtype=torch.float16), tensor([-2.4043, -0.1768,  2.7695, -0.7246, -4.4258,  1.8838, -7.4297, -4.7969,\n",
      "        -0.8564], device='cuda:5', dtype=torch.float16)]\n",
      "[tensor([3.8750, 3.5566, 3.0293, 4.8672, 3.4883, 3.0293, 5.3125, 5.3125, 2.8633],\n",
      "       device='cuda:5', dtype=torch.float16), tensor([-0.0820, -0.5884, -0.7192,  0.1123, -3.2520,  2.6816,  4.4180,  5.1953,\n",
      "         2.3848,  1.4971,  2.3184,  2.2637,  0.4941,  3.2656, -1.7236, -1.7959,\n",
      "        -2.0996, -0.2598, -0.3652, -2.0898, -1.7354, -0.6621], device='cuda:5',\n",
      "       dtype=torch.float16), tensor([ 1.9746,  2.8359,  2.8906,  2.4805, -0.1602,  2.9707, -0.8442, -0.1299,\n",
      "         2.3438], device='cuda:5', dtype=torch.float16)]\n",
      "clean metrics: 1.0\n",
      "corrupt metrics: 0.0\n"
     ]
    }
   ],
   "source": [
    "def ave_logit_difference(\n",
    "    logits: Float[Tensor, 'batch seq d_vocab'],\n",
    "    intervention_dataset,\n",
    "    per_prompt: bool = False\n",
    "):\n",
    "    batch_size = logits.size(0)\n",
    "    #print(f\"batch_size: {batch_size}\")\n",
    "    #print(f\"intervention_dataset.res_base_toks[:batch_size]: {intervention_dataset.res_base_toks[:batch_size]}\")\n",
    "    clean_logits = logits[range(batch_size), -1, intervention_dataset.res_base_toks[:batch_size]]\n",
    "    corrupt_logits = logits[range(batch_size), -1, intervention_dataset.pred_res_alt_toks[:batch_size]]\n",
    "    logit_diff = corrupt_logits - clean_logits\n",
    "    return logit_diff if per_prompt else logit_diff.mean()\n",
    "\n",
    "def process_batches_and_compute_logit_difference(model, intervention_dataset, tokens, per_prompt=True):\n",
    "    logit_differences = []\n",
    "    logits = []\n",
    "\n",
    "    # Iterate over each group of batches\n",
    "    for batch_idx, base_string_tok in enumerate(tokens):\n",
    "        with t.no_grad():\n",
    "            tok = t.vstack(base_string_tok)\n",
    "            logit = model(tok)\n",
    "            logit_diff = ave_logit_difference(logit, intervention_dataset, per_prompt)\n",
    "            logit_differences.append(logit_diff)\n",
    "            logits.append(logit)\n",
    "    return logits, logit_differences\n",
    "\n",
    "with t.no_grad():\n",
    "    clean_logits, clean_logit_diff = process_batches_and_compute_logit_difference(model, intervention_data,\n",
    "                                                                intervention_data.base_string_toks, per_prompt=True)\n",
    "    corrupt_logits, corrupt_logit_diff = process_batches_and_compute_logit_difference(model, intervention_data,\n",
    "                                                                intervention_data.alt_string_toks, per_prompt=True)\n",
    "    print(len(clean_logits[1]))\n",
    "    print(f\"clean_logits 1 shape {clean_logits[1].shape}\")\n",
    "    print(f\"clean_logits_1 diff {clean_logit_diff[1]}\")\n",
    "\n",
    "    #clean_logits = t.cat([t.flatten(diff) for diff in clean_logits])\n",
    "    #corrupt_logits = t.cat([t.flatten(diff) for diff in corrupt_logits])\n",
    "    #clean_logit_diff = t.mean(t.flatten(clean_logits)).item()\n",
    "    #corrupt_logit_diff = t.mean(t.flatten(corrupt_logits)).item()\n",
    "    print(clean_logit_diff)\n",
    "    print(corrupt_logit_diff)\n",
    "\n",
    "    def batched_metric(\n",
    "        logits: Float[Tensor, \"batch seq_len d_vocab\"],\n",
    "        corrupted_logit_diff = corrupt_logit_diff,\n",
    "        clean_logit_diff = clean_logit_diff,\n",
    "        intervention_dataset: intervention_data = intervention_data,\n",
    "        per_prompt: bool = True\n",
    "    ):\n",
    "        metrics = []\n",
    "        for batch_idx, logit in enumerate(logits):\n",
    "            patched_logit_diff = ave_logit_difference(logit, intervention_dataset, per_prompt)\n",
    "            metrics.append((patched_logit_diff - corrupted_logit_diff[batch_idx]) / (clean_logit_diff[batch_idx] - corrupted_logit_diff[batch_idx]))\n",
    "        \n",
    "        metrics = t.cat([t.flatten(metric) for metric in metrics])\n",
    "        mean_metric = t.mean(metrics)#.detach().cpu().item()\n",
    "\n",
    "        return mean_metric\n",
    "\n",
    "    with t.no_grad():\n",
    "        clean_metrics = batched_metric(clean_logits, corrupt_logit_diff, clean_logit_diff, intervention_data, per_prompt = True)\n",
    "        corrupt_metrics = batched_metric(corrupt_logits, corrupt_logit_diff, clean_logit_diff, intervention_data, per_prompt = True)\n",
    "        print(f\"clean metrics: {clean_metrics}\")\n",
    "        print(f\"corrupt metrics: {corrupt_metrics}\")\n",
    "    \n",
    "    \n",
    "# with t.no_grad():\n",
    "#     clean_logits = model(intervention_data.alt_string_toks)\n",
    "#     corrupt_logits = model(intervention_data.base_string_toks)\n",
    "#     clean_logit_diff = ave_logit_difference(clean_logits, intervention_data, per_prompt=False).item()\n",
    "#     corrupt_logit_diff = ave_logit_difference(corrupt_logits, intervention_data, per_prompt=False).item()\n",
    "# print(clean_logit_diff)\n",
    "# print(corrupt_logit_diff)\n",
    "\n",
    "def metric(\n",
    "    logits: Float[Tensor, \"batch seq_len d_vocab\"],\n",
    "    corrupted_logit_diff: float = corrupt_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    "    intervention_dataset: intervention_data = intervention_data,\n",
    "    per_prompt: bool = False\n",
    " ):\n",
    "    patched_logit_diff = ave_logit_difference(logits, intervention_dataset, per_prompt)\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)\n",
    "\n",
    "\n",
    "# Get clean and corrupt logit differences\n",
    "# with t.no_grad():\n",
    "#     print(f\"clean_logits metric shape {clean_logits.shape}\")\n",
    "#     clean_metric = metric(clean_logits, corrupt_logit_diff, clean_logit_diff, intervention_data, per_prompt = True)\n",
    "#     corrupt_metric = metric(corrupt_logits, corrupt_logit_diff, clean_logit_diff, intervention_data)\n",
    "\n",
    "# print(f'Clean direction: {clean_logit_diff}, Corrupt direction: {corrupt_logit_diff}')\n",
    "# print(f'Clean metric: {clean_metric}, Corrupt metric: {corrupt_metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1b9d4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean direction: 2.982421875, Corrupt direction: 2.83984375\n",
      "Clean metric: 1.0, Corrupt metric: 0.0\n"
     ]
    }
   ],
   "source": [
    "def ave_logit_diff(\n",
    "    logits: Float[Tensor, 'batch seq d_vocab'],\n",
    "    ioi_dataset: IOIDataset,\n",
    "    per_prompt: bool = False\n",
    "):\n",
    "    '''\n",
    "        Return average logit difference between correct and incorrect answers\n",
    "    '''\n",
    "    # Get logits for indirect objects\n",
    "    batch_size = logits.size(0)\n",
    "    # logits[batch_size, last_position, ID of IO]\n",
    "    io_logits = logits[range(batch_size), ioi_dataset.word_idx['end'][:batch_size], ioi_dataset.io_tokenIDs[:batch_size]]\n",
    "    s_logits = logits[range(batch_size), ioi_dataset.word_idx['end'][:batch_size], ioi_dataset.s_tokenIDs[:batch_size]]\n",
    "    # Get logits for subject\n",
    "    logit_diff = io_logits - s_logits\n",
    "    return logit_diff if per_prompt else logit_diff.mean()\n",
    "\n",
    "with t.no_grad():\n",
    "    clean_logits = model(clean_dataset.toks)\n",
    "    corrupt_logits = model(corr_dataset.toks)\n",
    "    clean_logit_diff = ave_logit_diff(clean_logits, clean_dataset).item() # logit difference for clean run between correct and incorrect answer\n",
    "    corrupt_logit_diff = ave_logit_diff(corrupt_logits, corr_dataset).item() # logit difference for corrupt run between correct and incorrect answer\n",
    "\n",
    "def ioi_metric(\n",
    "    logits: Float[Tensor, \"batch seq_len d_vocab\"],\n",
    "    corrupted_logit_diff: float = corrupt_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    "    ioi_dataset: IOIDataset = clean_dataset\n",
    " ):\n",
    "    patched_logit_diff = ave_logit_diff(logits, ioi_dataset)\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)\n",
    "\n",
    "def negative_ioi_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -ioi_metric(logits)\n",
    "    \n",
    "# Get clean and corrupt logit differences\n",
    "with t.no_grad():\n",
    "    clean_metric = ioi_metric(clean_logits, corrupt_logit_diff, clean_logit_diff, clean_dataset)\n",
    "    corrupt_metric = ioi_metric(corrupt_logits, corrupt_logit_diff, clean_logit_diff, corr_dataset)\n",
    "\n",
    "print(f'Clean direction: {clean_logit_diff}, Corrupt direction: {corrupt_logit_diff}')\n",
    "print(f'Clean metric: {clean_metric}, Corrupt metric: {corrupt_metric}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81ab6e",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving activations requires 0.0002 GB of memory per token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric <function ioi_metric at 0x7f645f1ec4c0>\n",
      "value: 1.0\n",
      "model config: HookedTransformerConfig:\n",
      "{'act_fn': 'gelu_new',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': ['global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local'],\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 64,\n",
      " 'd_mlp': 3072,\n",
      " 'd_model': 768,\n",
      " 'd_vocab': 50257,\n",
      " 'd_vocab_out': 50257,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': device(type='cuda'),\n",
      " 'dtype': torch.float16,\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.02886751345948129,\n",
      " 'model_name': 'gpt-neo-125M',\n",
      " 'n_ctx': 2048,\n",
      " 'n_devices': 5,\n",
      " 'n_heads': 12,\n",
      " 'n_key_value_heads': None,\n",
      " 'n_layers': 12,\n",
      " 'n_params': 84934656,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': 'GPTNeoForCausalLM',\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'post_embedding_ln': False,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000,\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'EleutherAI/gpt-neo-125M',\n",
      " 'tokenizer_prepends_bos': False,\n",
      " 'trust_remote_code': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': True,\n",
      " 'use_attn_scale': False,\n",
      " 'use_hook_mlp_in': True,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': True,\n",
      " 'use_split_qkv_input': True,\n",
      " 'window_size': 256}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp.0 -> [0.143] -> head.9.4.v\n",
      "mlp.0 -> [0.141] -> mlp.2\n",
      "head.0.2 -> [-0.098] -> mlp.0\n",
      "mlp.0 -> [-0.098] -> mlp.5\n",
      "head.0.2 -> [0.086] -> head.9.4.v\n",
      "head.1.11 -> [0.086] -> head.9.4.k\n",
      "mlp.0 -> [-0.081] -> head.9.4.k\n",
      "head.0.3 -> [-0.072] -> mlp.0\n",
      "head.0.2 -> [-0.067] -> mlp.2\n",
      "head.0.5 -> [0.061] -> head.9.4.v\n"
     ]
    }
   ],
   "source": [
    "model.reset_hooks()\n",
    "\n",
    "graph = EAP(\n",
    "    model,\n",
    "    clean_dataset.toks,\n",
    "    corr_dataset.toks,\n",
    "    ioi_metric,\n",
    "    upstream_nodes=[\"mlp\", \"head\"],\n",
    "    downstream_nodes=[\"mlp\", \"head\"],\n",
    "    batch_size=25\n",
    ")\n",
    "\n",
    "top_edges = graph.top_edges(n=10, abs_scores=True)\n",
    "for from_edge, to_edge, score in top_edges:\n",
    "    print(f'{from_edge} -> [{round(score, 3)}] -> {to_edge}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c32694c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 464, 1255,  286,  357, 1315,  532, 1478, 1267, 1635,  860,  796,  860,\n",
       "           13,  383, 1255,  286,  357, 1315,  532, 1478, 1267, 1635, 1105,  796],\n",
       "        [ 464, 1255,  286,  357,  807,  532,  767, 1267, 1635,  807,  796,  807,\n",
       "           13,  383, 1255,  286,  357,  678,  532, 1248, 1267, 1635,  362,  796],\n",
       "        [ 464, 1255,  286,  357, 1248,  532, 1596, 1267, 1635,  718,  796,  718,\n",
       "           13,  383, 1255,  286,  357,  718,  532,  642, 1267, 1635,  513,  796],\n",
       "        [ 464, 1255,  286,  357,  513,  532,  362, 1267, 1635, 1467,  796, 1467,\n",
       "           13,  383, 1255,  286,  357, 1160,  532,  678, 1267, 1635, 1467,  796],\n",
       "        [ 464, 1255,  286,  357, 1160,  532,  678, 1267, 1635,  718,  796,  718,\n",
       "           13,  383, 1255,  286,  357,  718,  532,  642, 1267, 1635, 1478,  796],\n",
       "        [ 464, 1255,  286,  357,  860,  532,  807, 1267, 1635,  860,  796,  860,\n",
       "           13,  383, 1255,  286,  357, 1511,  532, 1105, 1267, 1635,  362,  796],\n",
       "        [ 464, 1255,  286,  357, 1467,  532, 1478, 1267, 1635,  767,  796, 1478,\n",
       "           13,  383, 1255,  286,  357,  513,  532,  362, 1267, 1635, 1248,  796],\n",
       "        [ 464, 1255,  286,  357, 1467,  532, 1315, 1267, 1635,  604,  796,  604,\n",
       "           13,  383, 1255,  286,  357,  838,  532,  860, 1267, 1635, 1596,  796],\n",
       "        [ 464, 1255,  286,  357, 1367,  532,  838, 1267, 1635, 1315,  796, 1315,\n",
       "           13,  383, 1255,  286,  357, 1367,  532,  838, 1267, 1635, 1478,  796]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in intervention_data.base_string_toks:\n",
    "    base_tokens = t.vstack(i)\n",
    "for i in base_tokens:\n",
    "    i.to(device)\n",
    "base_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afa40907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving activations requires 0.0002 GB of memory per token\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_hooks()\n\u001b[0;32m----> 3\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mEAP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mintervention_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malt_string_toks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervention_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_string_toks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupstream_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhead\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownstream_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhead\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m top_edges \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mtop_edges(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, abs_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m from_edge, to_edge, score \u001b[38;5;129;01min\u001b[39;00m top_edges:\n",
      "File \u001b[0;32m~/dev/transformer/eap/edge-attribution-patching/eapp/eap_wrapper.py:124\u001b[0m, in \u001b[0;36mEAP\u001b[0;34m(model, clean_tokens, corrupted_tokens, metric, upstream_nodes, downstream_nodes, batch_size)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mEAP\u001b[39m(\n\u001b[1;32m    113\u001b[0m     model: HookedTransformer,\n\u001b[1;32m    114\u001b[0m     clean_tokens: Int[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size seq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    120\u001b[0m ):\n\u001b[1;32m    122\u001b[0m     graph \u001b[38;5;241m=\u001b[39m EAPGraph(model\u001b[38;5;241m.\u001b[39mcfg, upstream_nodes, downstream_nodes)\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m clean_tokens\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m \u001b[43mcorrupted_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch between clean and corrupted tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m     num_prompts, seq_len \u001b[38;5;241m=\u001b[39m clean_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], clean_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m num_prompts \u001b[38;5;241m%\u001b[39m batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of prompts must be divisible by batch size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model.reset_hooks()\n",
    "\n",
    "graph = EAP(\n",
    "    model,\n",
    "    intervention_data.alt_string_toks[0],\n",
    "    intervention_data.base_string_toks[0],\n",
    "    batched_metric,\n",
    "    upstream_nodes=[\"mlp\", \"head\"],\n",
    "    downstream_nodes=[\"mlp\", \"head\"],\n",
    "    batch_size=9\n",
    ")\n",
    "\n",
    "top_edges = graph.top_edges(n=10, abs_scores=True)\n",
    "for from_edge, to_edge, score in top_edges:\n",
    "    print(f'{from_edge} -> [{round(score, 3)}] -> {to_edge}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
