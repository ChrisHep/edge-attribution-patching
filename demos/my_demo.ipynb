{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aoq559/dev/transformer/eap/edge-attribution-patching\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58549/888417963.py:6: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"%load_ext autoreload\")\n",
      "/tmp/ipykernel_58549/888417963.py:7: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"%autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "%cd /home/aoq559/dev/transformer/eap/edge-attribution-patching\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "if ipython is not None:\n",
    "    ipython.magic(\"%load_ext autoreload\")\n",
    "    ipython.magic(\"%autoreload 2\")\n",
    "import torch\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import einops\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from eapp.eap_wrapper import EAP\n",
    "from jaxtyping import Float\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823a2a1c87fe4db2b54ab9679074c257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aoq559/miniconda3/envs/eap/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-12b-deduped-v0 into HookedTransformer\n",
      "Using tokenizer GPT2TokenizerFast(name_or_path='EleutherAI/gpt-neo-125m', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    'EleutherAI/pythia-12b-deduped-v0',\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    "    n_devices=7,\n",
    "    move_to_device=True,\n",
    "    dtype='float16'\n",
    ")\n",
    "# model = HookedTransformer.from_pretrained(\n",
    "#     'gpt-neo-125M',\n",
    "#     center_writing_weights=False,\n",
    "#     center_unembed=False,\n",
    "#     fold_ln=False,\n",
    "#     device=device,\n",
    "#     n_devices=5,\n",
    "#     move_to_device=True,\n",
    "#     dtype='float16'\n",
    "# )\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "model.tokenizer.padding_side = \"left\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "print(f\"Using tokenizer {tokenizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/shared-network/shared/2024_ml_master/data/EleutherAI/pythia-12b-deduped-v0/intervention_1_shots_max_20_arabic_further_templates.pkl\n",
      "Loaded data from /shared-network/shared/2024_ml_master/data/EleutherAI/pythia-12b-deduped-v0/intervention_1_shots_max_20_arabic_further_templates.pkl\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import yaml\n",
    "import pickle\n",
    "import os\n",
    "class DotDict(dict):\n",
    "    \"\"\" Dot notation access to dictionary attributes \"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "yaml_file_path = \"./conf/config.yaml\"\n",
    "with open(yaml_file_path, \"r\") as f:\n",
    "    args = DotDict(yaml.safe_load(f))\n",
    "\n",
    "file_name = args.data_dir\n",
    "file_name += '/' + str(args.model)\n",
    "file_name += '/intervention_' + str(args.n_shots) + '_shots_max_' + str(args.max_n) + '_' + args.representation\n",
    "file_name += '_further_templates' if args.extended_templates else ''\n",
    "file_name += '.pkl'\n",
    "print(file_name)\n",
    "\n",
    "with open(file_name, 'rb') as f:\n",
    "    intervention_list = pickle.load(f)\n",
    "print(\"Loaded data from\", file_name)\n",
    "if args.debug_run:\n",
    "    intervention_list = intervention_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intervention_dataset\n",
    "intervention_list = intervention_list\n",
    "intervention_data = intervention_dataset.InterventionDataset(intervention_list, device, model.tokenizer)\n",
    "intervention_data.create_intervention_dataset()\n",
    "intervention_data.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 104.00 MiB (GPU 0; 10.75 GiB total capacity; 1.28 GiB already allocated; 91.62 MiB free; 1.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m             all_logits\u001b[38;5;241m.\u001b[39mappend(logits)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mcat(all_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m clean_logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintervention_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_string_toks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintervention_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m corrupt_logits \u001b[38;5;241m=\u001b[39m logits_in_batches(model, intervention_data\u001b[38;5;241m.\u001b[39malt_string_toks, intervention_data\u001b[38;5;241m.\u001b[39mbase_attention_mask, \u001b[38;5;241m11\u001b[39m)\n\u001b[1;32m     30\u001b[0m clean_logit_diff \u001b[38;5;241m=\u001b[39m ave_logit_difference(clean_logits, intervention_data, per_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m, in \u001b[0;36mlogits_in_batches\u001b[0;34m(model, tokens, attn_mask, bsize)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m tokens[i:i\u001b[38;5;241m+\u001b[39mbsize]\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     20\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m attn_mask[i:i\u001b[38;5;241m+\u001b[39mbsize]\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 21\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/miniconda3/envs/eap/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/eap/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:562\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    559\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    560\u001b[0m         )\n\u001b[0;32m--> 562\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/miniconda3/envs/eap/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/eap/lib/python3.10/site-packages/transformer_lens/components.py:1434\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m   1432\u001b[0m     query_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_q_input(attn_in\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m   1433\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_k_input(attn_in\u001b[38;5;241m.\u001b[39mclone())\n\u001b[0;32m-> 1434\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_v_input(\u001b[43mattn_in\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1436\u001b[0m     query_input \u001b[38;5;241m=\u001b[39m attn_in\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 0; 10.75 GiB total capacity; 1.28 GiB already allocated; 91.62 MiB free; 1.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "def ave_logit_difference(\n",
    "    logits: Float[Tensor, 'batch seq d_vocab'],\n",
    "    intervention_dataset,\n",
    "    per_prompt: bool = False\n",
    "):\n",
    "    batch_size = logits.size(0)\n",
    "    clean_logits = logits[range(batch_size), -1, intervention_dataset.res_base_toks[:batch_size]]\n",
    "    corrupt_logits = logits[range(batch_size), -1, intervention_dataset.pred_res_alt_toks[:batch_size]]\n",
    "    logit_diff = corrupt_logits - clean_logits\n",
    "    return logit_diff if per_prompt else logit_diff.mean()\n",
    "\n",
    "def logits_in_batches(model, tokens, attn_mask, bsize):\n",
    "    model.eval()\n",
    "    seq_len = tokens.size(0)\n",
    "    all_logits = []\n",
    "\n",
    "    with t.no_grad():\n",
    "        for i in range(0, seq_len, bsize):\n",
    "            input = tokens[i:i+bsize].to(model.cfg.device)\n",
    "            attn_mask = attn_mask[i:i+bsize].to(model.cfg.device)\n",
    "            logits = model(input=input, attention_mask=attn_mask)\n",
    "            logits = logits.detach().cpu()\n",
    "            input = input.detach().cpu()\n",
    "            attn_mask = attn_mask.detach().cpu()\n",
    "            all_logits.append(logits)\n",
    "    return t.cat(all_logits, dim=0)\n",
    "\n",
    "clean_logits = logits_in_batches(model, intervention_data.base_string_toks, intervention_data.alt_attention_mask, 11)\n",
    "corrupt_logits = logits_in_batches(model, intervention_data.alt_string_toks, intervention_data.base_attention_mask, 11)\n",
    "clean_logit_diff = ave_logit_difference(clean_logits, intervention_data, per_prompt=False).item()\n",
    "corrupt_logit_diff = ave_logit_difference(corrupt_logits, intervention_data, per_prompt=False).item()\n",
    "print(clean_logit_diff)\n",
    "print(corrupt_logit_diff)\n",
    "    \n",
    "\n",
    "# with t.no_grad():\n",
    "#     clean_logits = model(intervention_data.base_string_toks, \n",
    "#                          attention_mask=intervention_data.alt_attention_mask)\n",
    "#     corrupt_logits = model(intervention_data.alt_string_toks,\n",
    "#                            attention_mask=intervention_data.base_attention_mask)\n",
    "#     clean_logit_diff = ave_logit_difference(clean_logits, intervention_data, per_prompt=False).item()\n",
    "#     corrupt_logit_diff = ave_logit_difference(corrupt_logits, intervention_data, per_prompt=False).item()\n",
    "# print(clean_logit_diff)\n",
    "# print(corrupt_logit_diff)\n",
    "\n",
    "def metric(\n",
    "    logits: Float[Tensor, \"batch seq_len d_vocab\"],\n",
    "    corrupted_logit_diff: float = corrupt_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    "    intervention_dataset: intervention_data = intervention_data,\n",
    "    per_prompt: bool = False\n",
    " ):\n",
    "    patched_logit_diff = ave_logit_difference(logits, intervention_dataset, per_prompt)\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model device cuda\n",
      "intervention_data device cuda\n",
      "attention_mask device cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9296875\n",
      "-2.642578125\n",
      "clean_logits metric shape torch.Size([40, 24, 50257])\n",
      "Clean direction: 1.9296875, Corrupt direction: -2.642578125\n",
      "Clean metric: 0.99951171875, Corrupt metric: 0.0\n"
     ]
    }
   ],
   "source": [
    "def ave_logit_difference(\n",
    "    logits: Float[Tensor, 'batch seq d_vocab'],\n",
    "    intervention_dataset,\n",
    "    per_prompt: bool = False\n",
    "):\n",
    "    batch_size = logits.size(0)\n",
    "    clean_logits = logits[range(batch_size), -1, intervention_dataset.res_base_toks[:batch_size]]\n",
    "    corrupt_logits = logits[range(batch_size), -1, intervention_dataset.pred_res_alt_toks[:batch_size]]\n",
    "    logit_diff = corrupt_logits - clean_logits\n",
    "    return logit_diff if per_prompt else logit_diff.mean()\n",
    "    \n",
    "    \n",
    "with t.no_grad():\n",
    "    clean_logits = model(intervention_data.alt_string_toks, \n",
    "                         attention_mask=intervention_data.alt_attention_mask)\n",
    "    corrupt_logits = model(intervention_data.base_string_toks,\n",
    "                           attention_mask=intervention_data.base_attention_mask)\n",
    "    clean_logit_diff = ave_logit_difference(clean_logits, intervention_data, per_prompt=False).item()\n",
    "    corrupt_logit_diff = ave_logit_difference(corrupt_logits, intervention_data, per_prompt=False).item()\n",
    "print(clean_logit_diff)\n",
    "print(corrupt_logit_diff)\n",
    "\n",
    "def metric(\n",
    "    logits: Float[Tensor, \"batch seq_len d_vocab\"],\n",
    "    corrupted_logit_diff: float = corrupt_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    "    intervention_dataset: intervention_data = intervention_data,\n",
    "    per_prompt: bool = False\n",
    " ):\n",
    "    patched_logit_diff = ave_logit_difference(logits, intervention_dataset, per_prompt)\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)\n",
    "\n",
    "\n",
    "#Get clean and corrupt logit differences\n",
    "with t.no_grad():\n",
    "    print(f\"clean_logits metric shape {clean_logits.shape}\")\n",
    "    clean_metric = metric(clean_logits, corrupt_logit_diff, clean_logit_diff, intervention_data, per_prompt = False)\n",
    "    corrupt_metric = metric(corrupt_logits, corrupt_logit_diff, clean_logit_diff, intervention_data, per_prompt = False)\n",
    "\n",
    "print(f'Clean direction: {clean_logit_diff}, Corrupt direction: {corrupt_logit_diff}')\n",
    "print(f'Clean metric: {clean_metric}, Corrupt metric: {corrupt_metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving activations requires 0.0002 GB of memory per token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:11<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp.0 -> [0.009] -> mlp.2\n",
      "mlp.0 -> [0.007] -> mlp.4\n",
      "Saving graph\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AGraph b'root' <Swig Object of type 'Agraph_t *' at 0x7feaa6dc3720>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.reset_hooks()\n",
    "\n",
    "graph = EAP(\n",
    "    model,\n",
    "    intervention_data.base_string_toks,\n",
    "    intervention_data.alt_string_toks,\n",
    "    metric,\n",
    "    upstream_nodes=[\"mlp\", \"head\"],\n",
    "    downstream_nodes=[\"mlp\", \"head\"],\n",
    "    batch_size=10,\n",
    "    alt_attention_mask=intervention_data.base_attention_mask,\n",
    "    base_attention_mask=intervention_data.alt_attention_mask\n",
    ")\n",
    "\n",
    "top_edges = graph.top_edges(n=2, abs_scores=True)\n",
    "for from_edge, to_edge, score in top_edges:\n",
    "    print(f'{from_edge} -> [{round(score, 3)}] -> {to_edge}')\n",
    "\n",
    "graph.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
