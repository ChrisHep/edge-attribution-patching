{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aoq559/dev/transformer/eap/edge-attribution-patching\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21581/1405079904.py:6: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"%load_ext autoreload\")\n",
      "/tmp/ipykernel_21581/1405079904.py:7: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"%autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "%cd /home/aoq559/dev/transformer/eap/edge-attribution-patching\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "if ipython is not None:\n",
    "    ipython.magic(\"%load_ext autoreload\")\n",
    "    ipython.magic(\"%autoreload 2\")\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import einops\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from eapp.eap_wrapper import EAP\n",
    "from jaxtyping import Float\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f6fe6f5baa4debbf53d08df08bfa35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aoq559/miniconda3/envs/eap/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-12b-deduped-v0 into HookedTransformer\n",
      "Using tokenizer GPT2TokenizerFast(name_or_path='EleutherAI/gpt-neo-125m', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    'EleutherAI/pythia-12b-deduped-v0',\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    "    n_devices=7,\n",
    "    move_to_device=True,\n",
    "    dtype='float16'\n",
    ")\n",
    "# model = HookedTransformer.from_pretrained(\n",
    "#     'gpt-neo-125M',\n",
    "#     center_writing_weights=False,\n",
    "#     center_unembed=False,\n",
    "#     fold_ln=False,\n",
    "#     device=device,\n",
    "#     n_devices=5,\n",
    "#     move_to_device=True,\n",
    "#     dtype='float16'\n",
    "# )\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "model.tokenizer.padding_side = \"left\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-12b-deduped-v0')\n",
    "print(f\"Using tokenizer {tokenizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from /shared-network/shared/2024_ml_master/data/EleutherAI/pythia-12b-deduped-v0/intervention_1_shots_max_20_arabic_further_templates_acdc.pkl\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import yaml\n",
    "import pickle\n",
    "import os\n",
    "class DotDict(dict):\n",
    "    \"\"\" Dot notation access to dictionary attributes \"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "yaml_file_path = \"./conf/config.yaml\"\n",
    "with open(yaml_file_path, \"r\") as f:\n",
    "    args = DotDict(yaml.safe_load(f))\n",
    "\n",
    "file_name = args.data_dir\n",
    "file_name += '/' + str(args.model)\n",
    "file_name += '/intervention_' + str(args.n_shots) + '_shots_max_' + str(args.max_n) + '_' + args.representation\n",
    "file_name += '_further_templates' if args.extended_templates else ''\n",
    "file_name += '_acdc' if args.acdc_data else ''\n",
    "file_name += '.pkl'\n",
    "\n",
    "with open(file_name, 'rb') as f:\n",
    "    intervention_list = pickle.load(f)\n",
    "print(\"Loaded data from\", file_name)\n",
    "if args.debug_run:\n",
    "    intervention_list = intervention_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intervention_dataset\n",
    "intervention_data = intervention_dataset.InterventionDataset(intervention_list, device, model.tokenizer)\n",
    "intervention_data.create_intervention_dataset()\n",
    "intervention_data.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.78173828125\n",
      "2.037109375\n",
      "Clean direction: -0.78173828125, Corrupt direction: 2.037109375\n",
      "Clean metric: 1.0, Corrupt metric: -0.0\n"
     ]
    }
   ],
   "source": [
    "def ave_logit_difference(\n",
    "    logits: Float[Tensor, 'batch seq d_vocab'],\n",
    "    intervention_dataset,\n",
    "    per_prompt: bool = False\n",
    "):\n",
    "    batch_size = logits.size(0)\n",
    "    clean_logits = logits[range(batch_size), -1, intervention_dataset.pred_res_alt_toks[:batch_size]]\n",
    "    corrupt_logits = logits[range(batch_size), -1, intervention_dataset.res_base_toks[:batch_size]]\n",
    "    logit_diff = corrupt_logits - clean_logits\n",
    "    return logit_diff if per_prompt else logit_diff.mean()\n",
    "\n",
    "def logits_in_batches(model, tokens, attn_mask, bsize):\n",
    "    model.eval()\n",
    "    seq_len = tokens.size(0)\n",
    "    all_logits = []\n",
    "\n",
    "    with t.no_grad():\n",
    "        for i in range(0, seq_len, bsize):\n",
    "            input = tokens[i:i+bsize].to(model.cfg.device)\n",
    "            attn_mask = attn_mask[i:i+bsize].to(model.cfg.device)\n",
    "            logits = model(input=input, attention_mask=attn_mask)\n",
    "            logits = logits.detach().cpu()\n",
    "            input = input.detach().cpu()\n",
    "            attn_mask = attn_mask.detach().cpu()\n",
    "            all_logits.append(logits)\n",
    "    return t.cat(all_logits, dim=0)\n",
    "\n",
    "clean_logits = logits_in_batches(model, intervention_data.alt_string_toks, intervention_data.base_attention_mask, 9)\n",
    "corrupt_logits = logits_in_batches(model, intervention_data.base_string_toks, intervention_data.alt_attention_mask, 9)\n",
    "clean_logit_diff = ave_logit_difference(clean_logits, intervention_data, per_prompt=False).item()\n",
    "corrupt_logit_diff = ave_logit_difference(corrupt_logits, intervention_data, per_prompt=False).item()\n",
    "print(clean_logit_diff)\n",
    "print(corrupt_logit_diff)\n",
    "\n",
    "def metric(\n",
    "    logits: Float[Tensor, \"batch seq_len d_vocab\"],\n",
    "    corrupted_logit_diff: float = corrupt_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    "    intervention_dataset: intervention_data = intervention_data,\n",
    "    per_prompt: bool = False\n",
    " ):\n",
    "    patched_logit_diff = ave_logit_difference(logits, intervention_dataset, per_prompt)\n",
    "    metric_result = (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)\n",
    "    return metric_result\n",
    "\n",
    "with t.no_grad():   \n",
    "    clean_metric = metric(clean_logits, corrupt_logit_diff, clean_logit_diff, intervention_data, per_prompt = False)\n",
    "    corrupt_metric = metric(corrupt_logits, corrupt_logit_diff, clean_logit_diff, intervention_data, per_prompt = False)\n",
    "\n",
    "print(f'Clean direction: {clean_logit_diff}, Corrupt direction: {corrupt_logit_diff}')\n",
    "print(f'Clean metric: {clean_metric}, Corrupt metric: {corrupt_metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ave_logit_difference(\n",
    "#     logits: Float[Tensor, 'batch seq d_vocab'],\n",
    "#     intervention_dataset,\n",
    "#     per_prompt: bool = False\n",
    "# ):\n",
    "#     batch_size = logits.size(0)\n",
    "#     clean_logits = logits[range(batch_size), -1, intervention_dataset.res_base_toks[:batch_size]]\n",
    "#     corrupt_logits = logits[range(batch_size), -1, intervention_dataset.pred_res_alt_toks[:batch_size]]\n",
    "#     logit_diff = corrupt_logits - clean_logits\n",
    "#     return logit_diff if per_prompt else logit_diff.mean()\n",
    "    \n",
    "    \n",
    "# with t.no_grad():\n",
    "#     clean_logits = model(intervention_data.alt_string_toks, \n",
    "#                          attention_mask=intervention_data.alt_attention_mask)\n",
    "#     corrupt_logits = model(intervention_data.base_string_toks,\n",
    "#                            attention_mask=intervention_data.base_attention_mask)\n",
    "#     clean_logit_diff = ave_logit_difference(clean_logits, intervention_data, per_prompt=False).item()\n",
    "#     corrupt_logit_diff = ave_logit_difference(corrupt_logits, intervention_data, per_prompt=False).item()\n",
    "# print(clean_logit_diff)\n",
    "# print(corrupt_logit_diff)\n",
    "\n",
    "# def metric(\n",
    "#     logits: Float[Tensor, \"batch seq_len d_vocab\"],\n",
    "#     corrupted_logit_diff: float = corrupt_logit_diff,\n",
    "#     clean_logit_diff: float = clean_logit_diff,\n",
    "#     intervention_dataset: intervention_data = intervention_data,\n",
    "#     per_prompt: bool = False\n",
    "#  ):\n",
    "#     patched_logit_diff = ave_logit_difference(logits, intervention_dataset, per_prompt)\n",
    "#     return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)\n",
    "\n",
    "\n",
    "# #Get clean and corrupt logit differences\n",
    "# with t.no_grad():\n",
    "#     print(f\"clean_logits metric shape {clean_logits.shape}\")\n",
    "#     clean_metric = metric(clean_logits, corrupt_logit_diff, clean_logit_diff, intervention_data, per_prompt = False)\n",
    "#     corrupt_metric = metric(corrupt_logits, corrupt_logit_diff, clean_logit_diff, intervention_data, per_prompt = False)\n",
    "\n",
    "# print(f'Clean direction: {clean_logit_diff}, Corrupt direction: {corrupt_logit_diff}')\n",
    "# print(f'Clean metric: {clean_metric}, Corrupt metric: {corrupt_metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving activations requires 0.0141 GB of memory per token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:30<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hookname: blocks.35.hook_mlp_in\n",
      "result shape: torch.Size([1475, 1])\n",
      "result_over_positions shape: torch.Size([24, 1475, 1])\n",
      "earlier_upstream_nodes_slice shape: slice(0, 1475, None)\n",
      "hook_slice shape: slice(4355, 4356, None)\n",
      "++++++++++++++++++++++++++++++++++++++\n",
      "torch.Size([1476, 4356])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_hooks()\n\u001b[0;32m----> 3\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mEAP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervention_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_string_toks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervention_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malt_string_toks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupstream_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhead\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownstream_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhead\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43malt_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintervention_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintervention_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malt_attention_mask\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m top_edges \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mtop_edges(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, abs_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m from_edge, to_edge, score \u001b[38;5;129;01min\u001b[39;00m top_edges:\n",
      "File \u001b[0;32m~/dev/transformer/eap/edge-attribution-patching/eapp/eap_wrapper.py:191\u001b[0m, in \u001b[0;36mEAP\u001b[0;34m(model, clean_tokens, corrupted_tokens, metric, upstream_nodes, downstream_nodes, batch_size, alt_attention_mask, base_attention_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m alt_attention_mask \u001b[38;5;241m=\u001b[39m alt_attention_mask\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    190\u001b[0m value \u001b[38;5;241m=\u001b[39m metric(model(clean_tokens[idx:idx\u001b[38;5;241m+\u001b[39mbatch_size], return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m, attention_mask\u001b[38;5;241m=\u001b[39malt_attention_mask[idx:idx\u001b[38;5;241m+\u001b[39mbatch_size]))\n\u001b[0;32m--> 191\u001b[0m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# We delete the activation differences tensor to free up memory\u001b[39;00m\n\u001b[1;32m    194\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/eap/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eap/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eap/lib/python3.10/site-packages/torch/utils/hooks.py:137\u001b[0m, in \u001b[0;36mBackwardHook._set_user_hook.<locals>.hook\u001b[0;34m(grad_input, _)\u001b[0m\n\u001b[1;32m    134\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pack_with_none(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_tensors_index, grad_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_inputs)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_hooks:\n\u001b[0;32m--> 137\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eap/lib/python3.10/site-packages/transformer_lens/hook_points.py:86\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/transformer/eap/edge-attribution-patching/eapp/eap_wrapper.py:88\u001b[0m, in \u001b[0;36mEAP_clean_backward_hook\u001b[0;34m(grad, hook, upstream_activations_difference, graph)\u001b[0m\n\u001b[1;32m     85\u001b[0m graph\u001b[38;5;241m.\u001b[39meap_scores \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39meap_scores\u001b[38;5;241m.\u001b[39mto(result\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(graph\u001b[38;5;241m.\u001b[39meap_scores\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 88\u001b[0m graph\u001b[38;5;241m.\u001b[39meap_scores[:, earlier_upstream_nodes_slice, hook_slice] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m result_over_positions\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "model.reset_hooks()\n",
    "\n",
    "graph = EAP(\n",
    "    model,\n",
    "    intervention_data.base_string_toks,\n",
    "    intervention_data.alt_string_toks,\n",
    "    metric,\n",
    "    upstream_nodes=[\"mlp\", \"head\"],\n",
    "    downstream_nodes=[\"mlp\", \"head\"],\n",
    "    batch_size=3,\n",
    "    alt_attention_mask=intervention_data.base_attention_mask,\n",
    "    base_attention_mask=intervention_data.alt_attention_mask\n",
    ")\n",
    "\n",
    "top_edges = graph.top_edges(n=20, abs_scores=True)\n",
    "for from_edge, to_edge, score in top_edges:\n",
    "    print(f'{from_edge} -> [{round(score, 3)}] -> {to_edge}')\n",
    "\n",
    "graph.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
